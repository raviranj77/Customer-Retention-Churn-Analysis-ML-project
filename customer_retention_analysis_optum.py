# -*- coding: utf-8 -*-
"""Customer Retention Analysis_OPTUM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h6z66Fx-p2Aw0RmT0hbOivGVvG-k2hQS

#Customer Retention Analysis Using Logistic Regression

#First Of All Import Libarires
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix,roc_auc_score, roc_curve
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

"""#Load Dataset"""

df=pd.read_excel("/content/drive/MyDrive/DATASET/customer_retention_dataset.xlsx")

df

"""#Intial CleanUP"""

df.replace(" ",np.nan,inplace= True)
df.dropna(inplace=True)

"""# Convert relevant columns to numeric"""

num_cols= ['Tenure', 'MonthlyCharges','TotalCharges']
for col in num_cols:
  df[col]=pd.to_numeric(df[col],errors='coerce')
df.dropna(subset=num_cols,inplace=True)

"""# Encode target variable 'Churn'"""

df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

"""# Drop non-informative column

"""

df.drop('CustomerID', axis=1, inplace=True)

"""# One-hot encode categorical variables

"""

df_encoded = pd.get_dummies(df, drop_first=True)

"""#Feature Selection & Train/Test Split

"""

X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#Feature Scaling"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""#Model Training - Logistic Regression

"""

model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

"""#Predictions & Probabilities

"""

y_pred = model.predict(X_test_scaled)
y_prob = model.predict_proba(X_test_scaled)[:, 1]

"""#Evaluation Metrics

"""

print("\nüìÑ Classification Report:\n", classification_report(y_test, y_pred))
print("\nüßæ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nüîç ROC-AUC Score:", roc_auc_score(y_test, y_prob))

"""#ROC Curve Visualization"""

fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label='Logistic Regression')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.tight_layout()
plt.show()

"""#Feature Importance Plot

"""

importance = pd.Series(model.coef_[0], index=X.columns).sort_values(key=abs, ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=importance[:15], y=importance.index[:15])
plt.title("Top 15 Features Influencing Customer Churn")
plt.xlabel("Coefficient Strength")
plt.tight_layout()
plt.show()

"""#Correlation Heatmap

"""

plt.figure(figsize=(12, 8))
corr = df_encoded.corr()
sns.heatmap(corr, cmap='coolwarm', annot=False, linewidths=0.5)
plt.title("Correlation Heatmap")
plt.tight_layout()
plt.show()

"""#Class Distribution

"""

sns.countplot(x='Churn', data=df)
plt.title("Distribution of Churn")
plt.xticks([0, 1], ['No', 'Yes'])
plt.tight_layout()
plt.show()

"""#Precision-Recall Curve (better for imbalanced data)"""

from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_test, y_prob)

plt.figure(figsize=(6, 4))
plt.plot(recall, precision, color='purple')
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.tight_layout()
plt.show()

"""#Handling Class Imbalance"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train_scaled, y_train)

"""#Top 10 customers most likely to churn:"""

X_test['Churn_Prob'] = y_prob
X_test['Actual'] = y_test.values
top_churn_risk = X_test.sort_values('Churn_Prob', ascending=False).head(10)
plt.figure(figsize=(10, 6))
sns.barplot(
    x='Churn_Prob',
    y=top_churn_risk.index,
    hue='Actual',
    data=top_churn_risk,
    palette='coolwarm'
)
plt.xlabel('Churn Probability')
plt.ylabel('Customer Index')
plt.title('Top 10 Customers at Risk of Churning')
plt.legend(title='Actual Churn')
plt.tight_layout()
plt.show()